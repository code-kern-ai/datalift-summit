{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../scraped_newsletters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and filtering the sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Running this we still got confirmation emails in the data, add a second filter!\n",
    "datascienceweekly_newsletter_df = df[(df[\"from\"].str.contains(\"datascienceweekly\")) & (True)].reset_index(drop=True)\n",
    "datascienceweekly_newsletter_df[\"newsletter\"] = \"datascienceweekly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Running this we still got a crypto newsletter in the data, add a second filter!\n",
    "tldr_newsletter_df = df[(df[\"from\"].str.contains(\"tldr\")) & (True)].reset_index(drop=True)\n",
    "tldr_newsletter_df[\"newsletter\"] = \"tldr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Running this we still got confirmation emails in the data, add a second filter!\n",
    "box_of_amazing_newsletter_df = df[(True) & (df[\"from\"] == \"rahim@rahimhirji.com\")].reset_index(drop=True)\n",
    "box_of_amazing_newsletter_df[\"newsletter\"] = \"box of amazing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the original HTMLs for later lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df = pd.concat([datascienceweekly_newsletter_df, tldr_newsletter_df, box_of_amazing_newsletter_df])[[\"newsletter\",\"date\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df.to_csv(\"html_lookup.csv\", index = False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the HTMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataScienceWeekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_datascienceweekly(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    \n",
    "    # TODO find all ul tags in \"data\"\n",
    "    all_uls = []\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for ul in all_uls:\n",
    "        # TODO get the font tag child from the ul tag\n",
    "        font = None\n",
    "\n",
    "        try:\n",
    "            children = list(font.children)\n",
    "            \n",
    "            # TODO extract the headline and the body\n",
    "            headline = \"\"\n",
    "            body = \"\"\n",
    "\n",
    "            processed_data.append({\n",
    "                \"newsletter\" : \"datascienceweekly\",\n",
    "                \"date\" : date,\n",
    "                \"headline\" : headline,\n",
    "                \"body\" : body\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if it works\n",
    "example = datascienceweekly_newsletter_df.sample(1)\n",
    "pd.DataFrame(get_split_stories_datascienceweekly(example[\"content\"].item(), example[\"date\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Finish this apply statement to convert the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = datascienceweekly_newsletter_df.apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"datascienceweekly_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_tldr(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    text_blocks = list(filter(lambda x: True if x.find(\"span\").find(\n",
    "        \"a\") else False, data.find_all(\"div\", {\"class\": \"text-block\"})))\n",
    "    # TODO check if all text_blocks make sense or if we want to drop some of them\n",
    "    text_blocks = text_blocks # filtering or is simple indexing enough?\n",
    "\n",
    "    articles = list(map(lambda x: {\n",
    "        \"newsletter\": \"TLDR\",\n",
    "        \"date\": date,\n",
    "        \"headline\": x.find(\"span\").find_all(\"span\")[0].text,\n",
    "        # TODO extract body in a similar matter to headline\n",
    "        \"body\": \"\"\n",
    "    }, text_blocks))\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO test if it works similar to the first newsletter\n",
    "example = None\n",
    "pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = tldr_newsletter_df.apply(lambda x: get_split_stories_tldr(x[\"content\"], x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"tldr_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box of Amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_box_of_amazing(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    # headlines are the <a>-tags ('cause they're links) with a certain styling on them, we get the headline texts by calling \".text\" on them\n",
    "    headlines = list(map(lambda x: x.text, data.find_all(\"a\",{\"style\":\"color: #3498DB; text-decoration: none;\"})))\n",
    "\n",
    "    # TODO look for similar attributes to extract the texts and find the relevant HTML tag\n",
    "    texts = list(map(lambda x: x.text, data.find_all()))\n",
    "\n",
    "    articles = []\n",
    "    for i in range(len(headlines)):\n",
    "        articles.append({\n",
    "            \"newsletter\":\"Box of Amazing\",\n",
    "            \"date\" : date,\n",
    "            \"headline\" : headlines[i],\n",
    "            \"body\" : texts[i].strip()\n",
    "        })\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if it works\n",
    "example = box_of_amazing_newsletter_df.sample(1)\n",
    "pd.DataFrame(get_split_stories_box_of_amazing(example[\"content\"].item(), example[\"date\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = box_of_amazing_newsletter_df.apply(lambda x: get_split_stories_box_of_amazing(x[\"content\"], x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"box_of_amazing_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_amazing_stories = pd.read_csv(\"box_of_amazing_stories.csv\", quoting = 1)\n",
    "tldr_stories = pd.read_csv(\"tldr_stories.csv\", quoting = 1)\n",
    "datascienceweekly_stories = pd.read_csv(\"datascienceweekly_stories.csv\", quoting = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories = pd.concat([box_of_amazing_stories, tldr_stories, datascienceweekly_stories])\n",
    "all_stories = all_stories.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories[\"ID\"] = list(all_stories.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories.to_csv(\"all_newsletter_stories.csv\", index = False, quoting = 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a8dd3a8ce1b4c991bd9fc20ecbd33bb3a991b4d95e67424ec48b6633f11a8d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('onetask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
