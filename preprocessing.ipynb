{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scraped_newsletters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and filtering the sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datascienceweekly_newsletter_df = df[(df[\"from\"].str.contains(\"datascienceweekly\")) & (df[\"subject\"].str.contains(\"Issue\"))].reset_index(drop=True)\n",
    "datascienceweekly_newsletter_df[\"newsletter\"] = \"datascienceweekly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tldr_newsletter_df = df[(df[\"from\"].str.contains(\"tldr\")) & (~df[\"from\"].str.contains(\"crypto\"))].reset_index(drop=True)\n",
    "tldr_newsletter_df[\"newsletter\"] = \"tldr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_amazing_newsletter_df = df[(df['date'] > '2022-04-01') & (df[\"from\"] == \"rahim@rahimhirji.com\")].reset_index(drop=True)\n",
    "box_of_amazing_newsletter_df[\"newsletter\"] = \"box of amazing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the original HTMLs for later lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "datascienceweekly_newsletter_df[[\"newsletter\",\"date\",\"content\"]].to_csv(\"datascienceweekly_lookup.csv\", index=False, quoting=1)\n",
    "tldr_newsletter_df[[\"newsletter\",\"date\",\"content\"]].to_csv(\"tldr_lookup.csv\", index=False, quoting=1)\n",
    "box_of_amazing_newsletter_df[[\"newsletter\",\"date\",\"content\"]].to_csv(\"box_of_amazing_lookup.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the HTMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataScienceWeekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_datascienceweekly(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    all_uls = data.find_all(\"ul\")\n",
    "    processed_data = []\n",
    "\n",
    "    for ul in all_uls:\n",
    "        # Necessary to the the previous tag and not filler, see https://www.crummy.com/software/BeautifulSoup/bs4/doc/#next-sibling-and-previous-sibling\n",
    "        previous = ul.previous_sibling.previous_sibling\n",
    "\n",
    "        # Get the topic header\n",
    "        if((previous.name == \"h2\") and (\"#34495e\" in previous[\"style\"])):\n",
    "            current_topic = previous.text.strip()\n",
    "\n",
    "        # Deprecated HTML tag, but they use it anyway\n",
    "        font = ul.find(\"font\")\n",
    "        try:\n",
    "            children = list(font.children)\n",
    "            headline = children[0].text.strip()\n",
    "            body = \" \".join([x.text.strip() for x in children[2:] if \"<br\" not in x])\n",
    "\n",
    "            # TODO maybe add the hyperlink to the article here\n",
    "            processed_data.append({\n",
    "                \"newsletter\" : \"datascienceweekly\",\n",
    "                \"date\" : date,\n",
    "                #\"topic\" : current_topic,\n",
    "                \"headline\" : headline,\n",
    "                \"body\" : body\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if it works\n",
    "example = datascienceweekly_newsletter_df.sample(1)\n",
    "pd.DataFrame(get_split_stories_datascienceweekly(example[\"content\"].item(), example[\"date\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = datascienceweekly_newsletter_df.apply(lambda x: get_split_stories_datascienceweekly(x[\"content\"], x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"datascienceweekly_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_tldr(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    text_blocks = list(filter(lambda x: True if x.find(\"span\").find(\n",
    "        \"a\") else False, data.find_all(\"div\", {\"class\": \"text-block\"})))\n",
    "    text_blocks = text_blocks[:-2]\n",
    "\n",
    "    articles = list(map(lambda x: {\n",
    "        \"newsletter\": \"TLDR\",\n",
    "        \"date\": date,\n",
    "        # \"topic\" : \"\",\n",
    "        \"headline\": x.find(\"span\").find_all(\"span\")[0].text,\n",
    "        \"body\": x.find(\"span\").find_all(\"span\")[1].text\n",
    "    }, text_blocks))\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if it works\n",
    "example = tldr_newsletter_df.sample(1)\n",
    "pd.DataFrame(get_split_stories_tldr(example[\"content\"].item(), example[\"date\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = tldr_newsletter_df.apply(lambda x: get_split_stories_tldr(x[\"content\"], x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"tldr_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box of Amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_stories_box_of_amazing(html_string, date):\n",
    "    data = BeautifulSoup(html_string, \"html.parser\")\n",
    "    headlines = list(map(lambda x: x.text, data.find_all(\"a\",{\"style\":\"color: #3498DB; text-decoration: none;\"})))\n",
    "    texts = list(map(lambda x: x.text, data.find_all(\"div\",{\"class\":\"link-description\"})))\n",
    "\n",
    "    articles = []\n",
    "    # TODO could also be done with zip\n",
    "    for i in range(len(headlines)):\n",
    "        articles.append({\n",
    "            \"newsletter\":\"Box of Amazing\",\n",
    "            \"date\" : date,\n",
    "            \"headline\" : headlines[i],\n",
    "            \"body\" : texts[i].strip()\n",
    "        })\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if it works\n",
    "example = box_of_amazing_newsletter_df.sample(1)\n",
    "pd.DataFrame(get_split_stories_box_of_amazing(example[\"content\"].item(), example[\"date\"].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the whole dataframe into the substory dataframe, which has one row for every full newsletter\n",
    "temp_df = box_of_amazing_newsletter_df.apply(lambda x: get_split_stories_box_of_amazing(x[\"content\"], x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the rows (which are lists of entries) into the actual final form\n",
    "final_df = pd.DataFrame([x for li in temp_df.ravel() for x in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again some random checking if everything looks good\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk\n",
    "final_df.to_csv(\"box_of_amazing_stories.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_amazing_stories = pd.read_csv(\"box_of_amazing_stories.csv\", quoting = 1)\n",
    "tldr_stories = pd.read_csv(\"tldr_stories.csv\", quoting = 1)\n",
    "datascienceweekly_stories = pd.read_csv(\"datascienceweekly_stories.csv\", quoting = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories = pd.concat([box_of_amazing_stories, tldr_stories, datascienceweekly_stories])\n",
    "all_stories = all_stories.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories[\"ID\"] = list(all_stories.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories.to_csv(\"all_newsletter_stories.csv\", index = False, quoting = 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a8dd3a8ce1b4c991bd9fc20ecbd33bb3a991b4d95e67424ec48b6633f11a8d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('onetask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
