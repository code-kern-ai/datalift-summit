{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedders.classification.contextual import TransformerSentenceEmbedder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Raw Data\n",
    "For the Model we use the embedders library which make embedding generation really easy to use. (alternatively you can for example use \"sentence-transformers\")\n",
    "\n",
    "We are also using the **kern export format** here, which is a simple json that can be read from pandas directly. \n",
    "\n",
    "If you're using a csv from an Excel export, just modify this code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedd the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations using Vector Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Topics covered by the article\n",
    "There are some problems when it comes to classifying topics in this setting. The most dominant one is that we don't know if the topics we selected are even all the topics that exist. For that we'd have to label every datapoint and also make sure that in the future there are no topics coming up that weren't in the training. Second, this data is rather unbalanced. We will not deal with these problems here and continue with our baseline usecase.\n",
    "\n",
    "Instead we will choose the topics that we want to have classified and which have enough support. We then introduce a \"catch-all\" class, where we map all other labels to.\n",
    "\n",
    "We will split the data into train and test set, train the model, and then evaluate it very quickly. We will not go into too much detail of the whole pipeline (it takes companies months to make sense of their data and models!) as this is not the aim of this workshop.\n",
    "\n",
    "After that we train on the whole available labeled data, predict with a set threshold and save the results to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train and Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate a classifier\n",
    "We can simulate the classification layer of a typical BERT pipeline with a LogisticRegression sklearn model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('onetask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a8dd3a8ce1b4c991bd9fc20ecbd33bb3a991b4d95e67424ec48b6633f11a8d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
